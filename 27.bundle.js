(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{111:function(t,e,n){"use strict";n.d(e,"a",(function(){return FeedDict})),n.d(e,"c",(function(){return p})),n.d(e,"b",(function(){return d}));var s=n(0),i=n(8),a=n(447),o=n(12),h=n(148),u=n(20);class FeedDict{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof FeedDict)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,e,n){if(null!=this.id2Value[t.id])throw new i.e(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=
/**
 * @license
 * Copyright 2018 Google LLC
 *
 * Use of this source code is governed by an MIT-style
 * license that can be found in the LICENSE file or at
 * https://opensource.org/licenses/MIT.
 * =============================================================================
 */
function(t,e){if(null==t.dtype||t.dtype===e.dtype)return e;try{return Object(s.Id)(e,t.dtype)}catch(n){throw new i.e(`The dtype of the feed (${e.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,e),this.name2Id[t.name]=t.id,null!=n&&(this.id2Mask[t.id]=n),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof u.d){if(null==this.id2Value[t.id])throw new i.e("Nonexistent key: "+t.name);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new i.e("Feed dict has no SymbolicTensor name: "+t);return this.id2Value[e]}}getMask(t){if(t instanceof u.d){if(null==this.id2Value[t.id])throw new i.e("Nonexistent key: "+t.name);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new i.e("Feed dict has no SymbolicTensor name: "+t);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&Object(s.Yd)(this.id2Mask)}}const r=new a.a,l=new a.a;function p(t){null!=r&&r.setMaxEntries(t),null!=l&&l.setMaxEntries(t)}function d(t,e,n,i){const a=null!=n&&n.training,u=Array.isArray(t),p=u?t:[t],d=p.map(t=>t.name),m=[],y=e.names();for(const t of d)-1!==y.indexOf(t)?m.push(e.getValue(t)):m.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const g=d.join(",")+"|"+e.names().sort().join(",");let w,I=r.get(g);if(null==I){const t=function(t,e){s.Hf.assert(null!=t&&t.length>0,()=>"Expected at least one fetch, got none");let n=[],i={};if(1===t.length){const s=f(t[0],e);n=s.sorted,i=s.recipientMap}else{const s=new Set;for(const a of t){const{sorted:t,recipientMap:o}=f(a,e);for(const e of t)s.has(e.name)||(n.push(e),s.add(e.name));for(const t in o)null==i[t]&&(i[t]=new Set),o[t].forEach(e=>i[t].add(e))}}return{sorted:n,recipientCounts:c(i)}}(p,e);I=t.sorted,w=t.recipientCounts,r.put(g,I),l.put(g,w)}w={},a||Object.assign(w,l.get(g));const S=new FeedDict(e);for(let t=0;t<I.length;++t){if(null!=i){const t=Object(s.Ee)().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const u=I[t],r=u.sourceLayer;if(r instanceof h.b)continue;const l=[],p=[],c=[];let f=!1;for(const t of u.inputs){const n=S.getValue(t),s=S.getMask(t);l.push(n),p.push(s),null!=s&&(f=!0),a||(w[t.name]--,0!==w[t.name]||e.hasKey(t)||-1!==d.indexOf(t.name)||n.isDisposed||!0===t.sourceLayer.stateful||c.push(n))}f&&((n=n||{}).mask=p[0]);const y=Object(o.o)(r.apply(l,n));let g=null;r.supportsMasking&&(g=r.computeMask(l,p));const N=b(u),L=Array.isArray(N)?N:[N];for(let t=0;t<L.length;++t){S.hasKey(L[t])||S.add(L[t],y[t],Array.isArray(g)?g[0]:g);const e=d.indexOf(L[t].name);-1!==e&&(m[e]=y[t])}a||Object(s.Yd)(c)}return S.disposeMasks(),u?m:m[0]}function c(t){const e={};for(const n in t)e[n]=t[n].size;return e}function f(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const a=[],o=[];for(a.push(t);a.length>0;){const t=a[a.length-1];if(n.has(t.name)){a.pop();continue}const e=o[o.length-1]===a.length-1;if(0===t.inputs.length||e)a.pop(),s.push(t),n.add(t.name),e&&o.pop();else{o.push(a.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||a.push(e)}}return{sorted:s,recipientMap:i}}function b(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}},148:function(t,e,n){"use strict";n.d(e,"b",(function(){return InputLayer})),n.d(e,"a",(function(){return h}));var s=n(0),i=n(136),a=n(8),o=n(20);
/**
 * @license
 * Copyright 2018 Google LLC
 *
 * Use of this source code is governed by an MIT-style
 * license that can be found in the LICENSE file or at
 * https://opensource.org/licenses/MIT.
 * =============================================================================
 */
class InputLayer extends o.b{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:Object(i.b)("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new a.e("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new a.e("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new a.e("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new o.d(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new o.c({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new a.e("Cannot pass any input to an InputLayer's apply() method. InputLayer name: "+this.name)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function h(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new a.e("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new InputLayer({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}InputLayer.className="InputLayer",s.df.registerClass(InputLayer)},20:function(t,e,n){"use strict";n.d(e,"a",(function(){return InputSpec})),n.d(e,"d",(function(){return SymbolicTensor})),n.d(e,"c",(function(){return Node})),n.d(e,"b",(function(){return Layer})),n.d(e,"e",(function(){return f}));var s=n(0),i=n(136),a=n(31),o=n(8),h=n(26),u=n(12),r=n(16),l=n(249),p=n(132);
/**
 * @license
 * Copyright 2018 Google LLC
 *
 * Use of this source code is governed by an MIT-style
 * license that can be found in the LICENSE file or at
 * https://opensource.org/licenses/MIT.
 * =============================================================================
 */
class InputSpec{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class SymbolicTensor{constructor(t,e,n,s,o,h,u){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=o,this.outputTensorIndex=u,this.id=Object(i.a)(),null!=h&&(this.originalName=Object(a.e)(h),this.name=Object(a.f)(this.originalName)),this.rank=e.length}}let d=0;class Node{constructor(t,e){this.callArgs=e,this.id=d++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let c=0;class Layer extends s.df.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=c++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=u.p(t)+"_"+Object(i.b)(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new o.d(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new o.e(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return u.m(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return u.m(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new o.b("Layer "+this.name+' has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use `getInputAt(nodeIndex)` instead.');if(0===this.inboundNodes.length)throw new o.b("Layer "+this.name+" is not connected, no input to return.");return u.m(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new o.b("Layer "+this.name+" has no inbound nodes.");if(this.inboundNodes.length>1)throw new o.b("Layer "+this.name+' has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use `getOutputAt(nodeIndex)` instead.');return u.m(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map(t=>t())}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach(e=>e.trainable=t),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter(t=>t.trainable):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter(t=>!t.trainable).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=u.o(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=u.o(this.inputSpec);if(t.length!==e.length)throw new o.e(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: `+t);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const a=s.rank;if(null!=i.ndim&&a!==i.ndim)throw new o.e(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${a}`);if(null!=i.maxNDim&&a>i.maxNDim)throw new o.e(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${a}`);if(null!=i.minNDim&&a<i.minNDim)throw new o.e(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${a}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new o.e(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),a=i.axes[e],h=s>=0?t[s]:t[t.length+s];if(null!=a&&-1===[a,null].indexOf(h))throw new o.e(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${a} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],a=s.shape[t];if(null!=e&&null!=a&&e!==a)throw new o.e(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=u.o(t);let s=!0;for(const t of n)if(!(t instanceof SymbolicTensor)){s=!1;break}let i=!0;for(const t of n)if(t instanceof SymbolicTensor){i=!1;break}if(s===i)throw new o.e("Arguments to apply() must be all SymbolicTensors or all Tensors");return Object(a.g)(this.name,()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of u.o(t))e.push(n.shape);this.build(u.m(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=u.o(s),a=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),a.push(t);if(s=u.m(a),null!=this.activityRegularizer)throw new o.c("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=u.o(t);const e=[];for(const n of t)e.push(n.shape);return u.m(e)}(t),s=this.computeOutputShape(n);let i;const a="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map((n,s)=>new SymbolicTensor(a,n,this,u.o(t),e,this.name,s)):new SymbolicTensor(a,s,this,u.o(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new o.c("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}})}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn("The rank of the input tensor provided (shape: "+JSON.stringify(t)+") does not match that of the "+`batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer `+this.name);else{let e=!1;this.batchInputShape.forEach((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)}),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: `+JSON.stringify(this.batchInputShape))}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new o.b(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new o.b(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new o.d(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return l.a(this.weights)}build(t){this.built=!0}getWeights(t=!1){return Object(p.b)(t?this.trainableWeights:this.weights)}setWeights(t){Object(s.Af)(()=>{const e=this.weights;if(e.length!==t.length)throw new o.e(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${e.length} weights. Provided weights: ${t}...`);if(0===e.length)return;const n=[],i=Object(p.b)(e);for(let a=0;a<i.length;++a){const h=i[a],u=e[a],r=t[a];if(!s.Hf.arraysEqual(h.shape,r.shape))throw new o.e(`Layer weight shape ${h.shape} not compatible with provided weight shape `+r.shape);n.push([u,r])}Object(p.c)(n)})}addWeight(t,e,n,s,i,a,u,r){if(-1!==this._addedWeightNames.indexOf(t))throw new o.e(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=null!=r?r():Object(h.q)("zeros"));const l=s.apply(e,n),d=new p.a(l,n,t,a,u);return l.dispose(),null!=i&&this.addLoss(()=>i.apply(d.read())),null==a&&(a=!0),a?this._trainableWeights.push(d):this._nonTrainableWeights.push(d),d}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=u.o(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach(t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)})}return null}return e}addInboundNode(t,e,n,s,i,a,o=null){const h=u.o(t);e=u.o(e),n=u.o(n),s=u.o(s),i=r.d(i),a=r.d(a);const l=[],p=[],d=[];for(const t of h)l.push(t.sourceLayer),p.push(t.nodeIndex),d.push(t.tensorIndex);new Node({outboundLayer:this,inboundLayers:l,nodeIndices:p,tensorIndices:d,inputTensors:h,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:a},o);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach(t=>t.dispose()),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function f(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=f(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}}}]);